{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train ner\n",
    "\n",
    "Example of training spaCy's named entity recognizer, starting off with an\n",
    "existing model or a blank model.\n",
    "\n",
    "For more details, see the documentation:\n",
    "* Training: https://spacy.io/usage/training\n",
    "* NER: https://spacy.io/usage/linguistic-features#named-entities\n",
    "\n",
    "Compatible with: spaCy v2.0.0+\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function\n",
    "\n",
    "import plac\n",
    "import random\n",
    "from pathlib import Path\n",
    "import spacy\n",
    "from spacy.util import minibatch, compounding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training data\n",
    "TRAIN_DATA = [\n",
    "    ('Who is Shaka Khan?', {\n",
    "        'entities': [(7, 17, 'PERSON')]\n",
    "    }),\n",
    "    ('I like London and Berlin.', {\n",
    "        'entities': [(7, 13, 'LOC'), (18, 24, 'LOC')]\n",
    "    })\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "@plac.annotations(\n",
    "    model=(\"Model name. Defaults to blank 'en' model.\", \"option\", \"m\", str),\n",
    "    output_dir=(\"Optional output directory\", \"option\", \"o\", Path),\n",
    "    n_iter=(\"Number of training iterations\", \"option\", \"n\", int))\n",
    "def main(model=None, output_dir=None, n_iter=100):\n",
    "    \"\"\"Load the model, set up the pipeline and train the entity recognizer.\"\"\"\n",
    "    if model is not None:\n",
    "        nlp = spacy.load(model)  # load existing spaCy model\n",
    "        print(\"Loaded model '%s'\" % model)\n",
    "    else:\n",
    "        nlp = spacy.blank('en')  # create blank Language class\n",
    "        print(\"Created blank 'en' model\")\n",
    "\n",
    "    # create the built-in pipeline components and add them to the pipeline\n",
    "    # nlp.create_pipe works for built-ins that are registered with spaCy\n",
    "    if 'ner' not in nlp.pipe_names:\n",
    "        ner = nlp.create_pipe('ner')\n",
    "        nlp.add_pipe(ner, last=True)\n",
    "    # otherwise, get it so we can add labels\n",
    "    else:\n",
    "        ner = nlp.get_pipe('ner')\n",
    "\n",
    "    # add labels\n",
    "    for _, annotations in TRAIN_DATA:\n",
    "        for ent in annotations.get('entities'):\n",
    "            ner.add_label(ent[2])\n",
    "\n",
    "    # get names of other pipes to disable them during training\n",
    "    other_pipes = [pipe for pipe in nlp.pipe_names if pipe != 'ner']\n",
    "    with nlp.disable_pipes(*other_pipes):  # only train NER\n",
    "        optimizer = nlp.begin_training()\n",
    "        for itn in range(n_iter):\n",
    "            random.shuffle(TRAIN_DATA)\n",
    "            losses = {}\n",
    "            # batch up the examples using spaCy's minibatch\n",
    "            batches = minibatch(TRAIN_DATA, size=compounding(4., 32., 1.001))\n",
    "            for batch in batches:\n",
    "                texts, annotations = zip(*batch)\n",
    "                nlp.update(\n",
    "                    texts,  # batch of texts\n",
    "                    annotations,  # batch of annotations\n",
    "                    drop=0.5,  # dropout - make it harder to memorise data\n",
    "                    sgd=optimizer,  # callable to update weights\n",
    "                    losses=losses)\n",
    "            print('Losses', losses)\n",
    "\n",
    "    # test the trained model\n",
    "    for text, _ in TRAIN_DATA:\n",
    "        doc = nlp(text)\n",
    "        print('Entities', [(ent.text, ent.label_) for ent in doc.ents])\n",
    "        print('Tokens', [(t.text, t.ent_type_, t.ent_iob) for t in doc])\n",
    "\n",
    "    # save model to output directory\n",
    "    if output_dir is not None:\n",
    "        output_dir = Path(output_dir)\n",
    "        if not output_dir.exists():\n",
    "            output_dir.mkdir()\n",
    "        nlp.to_disk(output_dir)\n",
    "        print(\"Saved model to\", output_dir)\n",
    "\n",
    "        # test the saved model\n",
    "        print(\"Loading from\", output_dir)\n",
    "        nlp2 = spacy.load(output_dir)\n",
    "        for text, _ in TRAIN_DATA:\n",
    "            doc = nlp2(text)\n",
    "            print('Entities', [(ent.text, ent.label_) for ent in doc.ents])\n",
    "            print('Tokens', [(t.text, t.ent_type_, t.ent_iob) for t in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created blank 'en' model\n",
      "Warning: Unnamed vectors -- this won't allow multiple vectors models to be loaded. (Shape: (0, 0))\n",
      "Losses {'ner': 2.522827895358205}\n",
      "Losses {'ner': 3.079001933336258}\n",
      "Losses {'ner': 2.8131771609187126}\n",
      "Losses {'ner': 4.283849477576496}\n",
      "Losses {'ner': 2.2743456019088626}\n",
      "Losses {'ner': 2.5707041394029275}\n",
      "Losses {'ner': 2.262006998062134}\n",
      "Losses {'ner': 3.1667595505714417}\n",
      "Losses {'ner': 1.9905483856351225}\n",
      "Losses {'ner': 2.5009699231013656}\n",
      "Losses {'ner': 2.2931356728076935}\n",
      "Losses {'ner': 1.5065250825596195}\n",
      "Losses {'ner': 1.2815003036521375}\n",
      "Losses {'ner': 2.212900996208191}\n",
      "Losses {'ner': 0.9618368346311925}\n",
      "Losses {'ner': 0.7942630067179}\n",
      "Losses {'ner': 1.2888246915534771}\n",
      "Losses {'ner': 1.7230192851276023}\n",
      "Losses {'ner': 1.2611259520053864}\n",
      "Losses {'ner': 2.0093413492518266}\n",
      "Losses {'ner': 1.933947324755831}\n",
      "Losses {'ner': 0.7821112492410057}\n",
      "Losses {'ner': 0.5163920576908293}\n",
      "Losses {'ner': 1.8708034353330731}\n",
      "Losses {'ner': 0.12890530865832373}\n",
      "Losses {'ner': 0.46198916713282395}\n",
      "Losses {'ner': 1.4515597598631826}\n",
      "Losses {'ner': 8.429262355487156e-11}\n",
      "Losses {'ner': 0.07635215093324632}\n",
      "Losses {'ner': 0.8024681420128521}\n",
      "Losses {'ner': 0.01879896783521362}\n",
      "Losses {'ner': 0.4998168060131312}\n",
      "Losses {'ner': 1.3130485458323164}\n",
      "Losses {'ner': 1.0653180711483343e-07}\n",
      "Losses {'ner': 1.1000694194908643e-06}\n",
      "Losses {'ner': 8.734062631241005e-07}\n",
      "Losses {'ner': 0.6146823333743273}\n",
      "Losses {'ner': 0.11240279691895694}\n",
      "Losses {'ner': 0.3291702674604551}\n",
      "Losses {'ner': 0.10140999335946181}\n",
      "Losses {'ner': 0.1485348663440582}\n",
      "Losses {'ner': 0.005936962028743265}\n",
      "Losses {'ner': 0.00011224390803710317}\n",
      "Losses {'ner': 3.557773115014668e-12}\n",
      "Losses {'ner': 0.3946864717247066}\n",
      "Losses {'ner': 2.276843111507333e-07}\n",
      "Losses {'ner': 0.020134281371513293}\n",
      "Losses {'ner': 3.374611127068428e-10}\n",
      "Losses {'ner': 1.0639598501793269e-07}\n",
      "Losses {'ner': 1.11102538771885e-06}\n",
      "Losses {'ner': 4.5100377624143524e-11}\n",
      "Losses {'ner': 0.007270305146845863}\n",
      "Losses {'ner': 4.7629540337695364e-09}\n",
      "Losses {'ner': 4.2772855182048886e-08}\n",
      "Losses {'ner': 6.143756357922329e-06}\n",
      "Losses {'ner': 5.11624647276652e-12}\n",
      "Losses {'ner': 4.011659257230349e-05}\n",
      "Losses {'ner': 1.527607909391084e-08}\n",
      "Losses {'ner': 0.4694985890830226}\n",
      "Losses {'ner': 1.8509828844716177e-05}\n",
      "Losses {'ner': 3.901704433009061e-16}\n",
      "Losses {'ner': 3.361535760871036e-08}\n",
      "Losses {'ner': 0.4138634320905803}\n",
      "Losses {'ner': 6.855862806331587e-18}\n",
      "Losses {'ner': 3.4114361321585554e-18}\n",
      "Losses {'ner': 8.250354367733885e-15}\n",
      "Losses {'ner': 8.08880269960922e-11}\n",
      "Losses {'ner': 5.207148967583431e-15}\n",
      "Losses {'ner': 5.417283303150599e-18}\n",
      "Losses {'ner': 1.876194558368862e-10}\n",
      "Losses {'ner': 6.468090856328472e-08}\n",
      "Losses {'ner': 5.599288940716601e-12}\n",
      "Losses {'ner': 1.0161948011964828e-06}\n",
      "Losses {'ner': 1.858414709343554e-16}\n",
      "Losses {'ner': 1.086065427880739e-08}\n",
      "Losses {'ner': 2.4257881065091014e-08}\n",
      "Losses {'ner': 4.2918845720721554e-17}\n",
      "Losses {'ner': 2.4043369725725655e-11}\n",
      "Losses {'ner': 0.0048158415607815015}\n",
      "Losses {'ner': 1.4912152687969566e-17}\n",
      "Losses {'ner': 4.947896317675215e-06}\n",
      "Losses {'ner': 3.26862772216379e-08}\n",
      "Losses {'ner': 1.540140940775581e-17}\n",
      "Losses {'ner': 1.6217053792735647e-12}\n",
      "Losses {'ner': 9.738671604204046e-15}\n",
      "Losses {'ner': 1.6938431102765658e-08}\n",
      "Losses {'ner': 9.184741464842447e-11}\n",
      "Losses {'ner': 1.9155876797933666e-17}\n",
      "Losses {'ner': 3.9614590718883846e-08}\n",
      "Losses {'ner': 1.8991444701592313e-12}\n",
      "Losses {'ner': 2.619479576974693e-16}\n",
      "Losses {'ner': 2.865400094647436e-21}\n",
      "Losses {'ner': 5.009665643642584e-13}\n",
      "Losses {'ner': 4.039968569322414e-10}\n",
      "Losses {'ner': 7.624229026773596e-11}\n",
      "Losses {'ner': 9.717729594106717e-11}\n",
      "Losses {'ner': 0.496226280972152}\n",
      "Losses {'ner': 1.017797883701511e-19}\n",
      "Losses {'ner': 4.5507016344436804e-14}\n",
      "Losses {'ner': 1.7951273650742106e-27}\n",
      "Entities [('Shaka Khan', 'PERSON')]\n",
      "Tokens [('Who', '', 2), ('is', '', 2), ('Shaka', 'PERSON', 3), ('Khan', 'PERSON', 1), ('?', '', 2)]\n",
      "Entities [('London', 'LOC'), ('Berlin', 'LOC')]\n",
      "Tokens [('I', '', 2), ('like', '', 2), ('London', 'LOC', 3), ('and', '', 2), ('Berlin', 'LOC', 3), ('.', '', 2)]\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Expected output:\n",
    "# Entities [('Shaka Khan', 'PERSON')]\n",
    "# Tokens [('Who', '', 2), ('is', '', 2), ('Shaka', 'PERSON', 3),\n",
    "# ('Khan', 'PERSON', 1), ('?', '', 2)]\n",
    "# Entities [('London', 'LOC'), ('Berlin', 'LOC')]\n",
    "# Tokens [('I', '', 2), ('like', '', 2), ('London', 'LOC', 3),\n",
    "# ('and', '', 2), ('Berlin', 'LOC', 3), ('.', '', 2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
