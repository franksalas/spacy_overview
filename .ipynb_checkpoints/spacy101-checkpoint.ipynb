{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " Binary weights for the part-of-speech tagger, dependency parser and named entity recognizer to predict those annotations in context.\n",
    " Lexical entries in the vocabulary, i.e. words and their context-independent attributes like the shape or spelling.\n",
    " Word vectors, i.e. multi-dimensional meaning representations of words that let you determine how similar they are to each other.\n",
    " Configuration options, like the language and processing pipeline settings, to put spaCy in the correct state when you load in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en')\n",
    "doc = nlp(u'This is a sentence.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function Doc.print_tree>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc.print_tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features\n",
    "\n",
    "\n",
    "- Tokenization: Segmenting text into words, punctuations marks etc.\n",
    "- Part-of-speech (POS) Tagging\tAssigning word types to tokens, like verb or noun.\n",
    "- Dependency Parsing:\tAssigning syntactic dependency labels, describing the relations between individual tokens, like subject or object.\n",
    "- Lemmatization:\tAssigning the base forms of words. For example, the lemma of \"was\" is \"be\", and the lemma of \"rats\" is \"rat\".\n",
    "- Sentence Boundary Detection (SBD):\tFinding and segmenting individual sentences.\n",
    "- Named Entity Recognition (NER):\tLabelling named \"real-world\" objects, like persons, companies or locations.\n",
    "- Similarity:\tComparing words, text spans and documents and how similar they are to each other.\n",
    "- Text Classification:\tAssigning categories or labels to a whole document, or parts of a document.\n",
    "- Rule-based Matching:\tFinding sequences of tokens based on their texts and linguistic annotations, similar to regular expressions.\n",
    "- Training:\tUpdating and improving a statistical model's predictions.\n",
    "- Serialization: Saving objects to file or byte strings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistical models\n",
    "While some of spaCy's features work independently, others require statistical models to be loaded, which enable spaCy to predict linguistic annotations – for example, whether a word is a verb or a noun. spaCy currently offers statistical models for 8 languages, which can be installed as individual Python modules. Models can differ in size, speed, memory usage, accuracy and the data they include. The model you choose always depends on your use case and the texts you're working with. For a general-purpose use case, the small, default models are always a good start. They typically include the following components:\n",
    "\n",
    "\n",
    "\n",
    "-  **Binary weights** for the part-of-speech tagger, dependency parser and named entity recognizer to predict those annotations in context.\n",
    "- **Lexical entries** in the vocabulary, i.e. words and their context-independent attributes like the shape or spelling.\n",
    "- **Word vectors**, i.e. multi-dimensional meaning representations of words that let you determine how similar they are to each other.\n",
    "- **Configuration** options, like the language and processing pipeline settings, to put spaCy in the correct state when you load in the model.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linguistic annotations\n",
    "spaCy provides a variety of linguistic annotations to give you insights into a text's grammatical structure. This includes the word types, like the parts of speech, and how the words are related to each other. For example, if you're analysing text, it makes a huge difference whether a noun is the subject of a sentence, or the object – or whether \"google\" is used as a verb, or refers to the website or company in a specific context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "doc = nlp(u'Apple is looking at buying U.K. startup for $1 billion')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token.text\t token.pos_\t token.dep_\n",
      "Apple\t\tPROPN\t\tnsubj\n",
      "is\t\tVERB\t\taux\n",
      "looking\t\tVERB\t\tROOT\n",
      "at\t\tADP\t\tprep\n",
      "buying\t\tVERB\t\tpcomp\n",
      "U.K.\t\tPROPN\t\tcompound\n",
      "startup\t\tNOUN\t\tdobj\n",
      "for\t\tADP\t\tprep\n",
      "$\t\tSYM\t\tquantmod\n",
      "1\t\tNUM\t\tcompound\n",
      "billion\t\tNUM\t\tpobj\n"
     ]
    }
   ],
   "source": [
    "print('token.text\\t token.pos_\\t token.dep_')\n",
    "for token in doc:\n",
    "    print(\"{}\\t\\t{}\\t\\t{}\".format(token.text, token.pos_, token.dep_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "During processing, spaCy first tokenizes the text, i.e. segments it into words, punctuation and so on. This is done by applying rules specific to each language. For example, punctuation at the end of a sentence should be split off – whereas \"U.K.\" should remain one token. Each Doc consists of individual tokens, and we can iterate over them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple\n",
      "is\n",
      "looking\n",
      "at\n",
      "buying\n",
      "U.K.\n",
      "startup\n",
      "for\n",
      "$\n",
      "1\n",
      "billion\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(token.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, the raw text is split on whitespace characters, similar to text.split(' '). Then, the tokenizer processes the text from left to right. On each substring, it performs two checks:\n",
    "\n",
    " Does the substring match a tokenizer exception rule? For example, \"don't\" does not contain whitespace, but should be split into two tokens, \"do\" and \"n't\", while \"U.K.\" should always remain one token.\n",
    " Can a prefix, suffix or infix be split off? For example punctuation like commas, periods, hyphens or quotes.\n",
    "If there's a match, the rule is applied and the tokenizer continues its loop, starting with the newly split substrings. This way, spaCy can split complex, nested tokens like combinations of abbreviations and multiple punctuation marks.\n",
    "\n",
    "\n",
    "```\n",
    "Tokenizer exception: Special-case rule to split a string into several tokens or prevent a token from being split when punctuation rules are applied.\n",
    "Prefix: Character(s) at the beginning, e.g. $, (, “, ¿.\n",
    "Suffix: Character(s) at the end, e.g. km, ), ”, !.\n",
    "Infix: Character(s) in between, e.g. -, --, /, ….\n",
    "```\n",
    "\n",
    "![](https://spacy.io/assets/img/tokenization.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part-of-speech tags and dependencies\n",
    "\n",
    "After tokenization, spaCy can **parse** and **tag** a given Doc. This is where the statistical model comes in, which enables spaCy to make a prediction of which tag or label most likely applies in this context. A model consists of binary data and is produced by showing a system enough examples for it to make predictions that generalise across the language – for example, a word following \"the\" in English is most likely a noun.\n",
    "\n",
    "Linguistic annotations are available as Token attributes . Like many NLP libraries, spaCy encodes all strings to hash values to reduce memory usage and improve efficiency. So to get the readable string representation of an attribute, we need to add an underscore _ to its name:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEXT\t LEMMA\t POS\t TAG\t DEP\t SHAPE\t ALPHA\t STOP\n",
      "Apple\tapple\tPROPN\tNNP\tnsubj\tXxxxx\tTrue\tFalse\n",
      "is\tbe\tVERB\tVBZ\taux\txx\tTrue\tTrue\n",
      "looking\tlook\tVERB\tVBG\tROOT\txxxx\tTrue\tFalse\n",
      "at\tat\tADP\tIN\tprep\txx\tTrue\tTrue\n",
      "buying\tbuy\tVERB\tVBG\tpcomp\txxxx\tTrue\tFalse\n",
      "U.K.\tu.k.\tPROPN\tNNP\tcompound\tX.X.\tFalse\tFalse\n",
      "startup\tstartup\tNOUN\tNN\tdobj\txxxx\tTrue\tFalse\n",
      "for\tfor\tADP\tIN\tprep\txxx\tTrue\tTrue\n",
      "$\t$\tSYM\t$\tquantmod\t$\tFalse\tFalse\n",
      "1\t1\tNUM\tCD\tcompound\td\tFalse\tFalse\n",
      "billion\tbillion\tNUM\tCD\tpobj\txxxx\tTrue\tFalse\n"
     ]
    }
   ],
   "source": [
    "print(\"TEXT\\t LEMMA\\t POS\\t TAG\\t DEP\\t SHAPE\\t ALPHA\\t STOP\")\n",
    "for token in doc:\n",
    "    print(\"{}\\t{}\\t{}\\t{}\\t{}\\t{}\\t{}\\t{}\".format(token.text, token.lemma_, token.pos_, \n",
    "                                                  token.tag_, token.dep_,token.shape_, \n",
    "                                                  token.is_alpha, token.is_stop))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Named Entities \n",
    "A named entity is a \"real-world object\" that's assigned a name – for example, a person, a country, a product or a book title. spaCy can recognise various types of named entities in a document, by asking the model for a prediction. Because models are statistical and strongly depend on the examples they were trained on, this doesn't always work perfectly and might need some tuning later, depending on your use case.\n",
    "\n",
    "Named entities are available as the ents property of a Doc:\n",
    "\n",
    "\n",
    "```\n",
    "Text: The original entity text.\n",
    "Start: Index of start of entity in the Doc.\n",
    "End: Index of end of entity in the Doc.\n",
    "Label: Entity label, i.e. type.\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple|\t0|\t   5\n",
      "U.K.|\t27|\t   31\n",
      "$1 billion|\t44|\t   54\n"
     ]
    }
   ],
   "source": [
    "for ent in doc.ents:\n",
    "    print('{}|\\t{}|\\t   {}'.format(ent.text, ent.start_char, ent.end_char, ent.label_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word vectors and similarityNEEDS MODEL \n",
    "spaCy is able to compare two objects, and make a prediction of how similar they are. Predicting similarity is useful for building recommendation systems or flagging duplicates. For example, you can suggest a user content that's similar to what they're currently looking at, or label a support ticket as a duplicate if it's very similar to an already existing one.\n",
    "\n",
    "Each Doc, Span and Token comes with a .similarity()  method that lets you compare it with another object, and determine the similarity. Of course similarity is always subjective – whether \"dog\" and \"cat\" are similar really depends on how you're looking at it. spaCy's similarity model usually assumes a pretty general-purpose definition of similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dog\tdog\t1.00\n",
      "dog\tcat\t0.80\n",
      "dog\tbanana\t0.24\n",
      "cat\tdog\t0.80\n",
      "cat\tcat\t1.00\n",
      "cat\tbanana\t0.28\n",
      "banana\tdog\t0.24\n",
      "banana\tcat\t0.28\n",
      "banana\tbanana\t1.00\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_lg')  # make sure to use larger model!\n",
    "tokens = nlp(u'dog cat banana')\n",
    "\n",
    "for token1 in tokens:\n",
    "    for token2 in tokens:\n",
    "        print('{}\\t{}\\t{:1.2f}'.format(token1.text, token2.text, token1.similarity(token2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Models that come with built-in word vectors make them available as the Token.vector  attribute. Doc.vector  and Span.vector  will default to an average of their token vectors. You can also check if a token has a vector assigned, and get the L2 norm, which can be used to normalise vectors.\n",
    "\n",
    "```\n",
    "Text: The original token text.\n",
    "has vector: Does the token have a vector representation?\n",
    "Vector norm: The L2 norm of the token's vector (the square root of the sum of the values squared)\n",
    "OOV: Out-of-vocabulary\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text\tvector\tvec_norm\tis_ovv\n",
      "dog\tTrue\t7.03\t\tFalse\n",
      "cat\tTrue\t6.68\t\tFalse\n",
      "banana\tTrue\t6.70\t\tFalse\n",
      "afskfsd\tFalse\t0.00\t\tTrue\n"
     ]
    }
   ],
   "source": [
    "tokens = nlp(u'dog cat banana afskfsd')\n",
    "print('text\\tvector\\tvec_norm\\tis_ovv')\n",
    "for token in tokens:\n",
    "    print('{}\\t{}\\t{:1.2f}\\t\\t{}'.format(token.text, token.has_vector, token.vector_norm, token.is_oov))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The words \"dog\", \"cat\" and \"banana\" are all pretty common in English, so they're part of the model's vocabulary, and come with a vector. The word \"afskfsd\" on the other hand is a lot less common and out-of-vocabulary – so its vector representation consists of 300 dimensions of 0, which means it's practically nonexistent. If your application will benefit from a large vocabulary with more vectors, you should consider using one of the larger models or loading in a full vector package, for example, en_vectors_web_lg, which includes over 1 million unique vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipelines\n",
    "\n",
    "When you call nlp on a text, spaCy first tokenizes the text to produce a Doc object. The Doc is then processed in several different steps – this is also referred to as the processing pipeline. The pipeline used by the default models consists of a tagger, a parser and an entity recognizer. Each pipeline component returns the processed Doc, which is then passed on to the next component.\n",
    "\n",
    "![](https://spacy.io/assets/img/pipeline.svg)\n",
    "\n",
    "\n",
    "\n",
    "![](https://i.imgur.com/p8nlUeU.png)\n",
    "\n",
    "\n",
    "The processing pipeline always depends on the statistical model and its capabilities. For example, a pipeline can only include an entity recognizer component if the model includes data to make predictions of entity labels. This is why each model will specify the pipeline to use in its meta data, as a simple list containing the component names:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vocab, hashes and lexemes\n",
    "\n",
    "Whenever possible, spaCy tries to store data in a vocabulary, the Vocab , that will be shared by multiple documents. To save memory, spaCy also encodes all strings to hash values – in this case for example, \"coffee\" has the hash 3197928453018144401. Entity labels like \"ORG\" and part-of-speech tags like \"VERB\" are also encoded. Internally, spaCy only \"speaks\" in hash values.\n",
    "\n",
    "\n",
    "![](https://spacy.io/assets/img/vocab_stringstore.svg)\n",
    "\n",
    "\n",
    "If you process lots of documents containing the word \"coffee\" in all kinds of different contexts, storing the exact string \"coffee\" every time would take up way too much space. So instead, spaCy hashes the string and stores it in the StringStore . You can think of the StringStore as a lookup table that works in both directions – you can look up a string to get its hash, or a hash to get its string:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3197928453018144401\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(u'I love coffee')\n",
    "vocab_string = doc.vocab.strings['coffee']\n",
    "\n",
    "print(vocab_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'coffee'"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc.vocab.strings[vocab_string]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that all strings are encoded, the entries in the vocabulary don't need to include the word text themselves. Instead, they can look it up in the StringStore via its hash value. Each entry in the vocabulary, also called Lexeme , contains the context-independent information about a word. For example, no matter if \"love\" is used as a verb or a noun in some context, its spelling and whether it consists of alphabetic characters won't ever change. Its hash value will also always be the sam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I\t4690420944186131903\tX\tI\tI\tTrue\tFalse\tTrue\ten\n",
      "love\t3702023516439754181\txxxx\tl\tove\tTrue\tFalse\tFalse\ten\n",
      "coffee\t3197928453018144401\txxxx\tc\tfee\tTrue\tFalse\tFalse\ten\n"
     ]
    }
   ],
   "source": [
    "doc = nlp('I love coffee')\n",
    "for word in doc:\n",
    "    lexeme = doc.vocab[word.text]\n",
    "    \n",
    "    print('{}\\t{}\\t{}\\t{}\\t{}\\t{}\\t{}\\t{}\\t{}'.format(lexeme.text, lexeme.orth, lexeme.shape_, lexeme.prefix_, lexeme.suffix_,\n",
    "          lexeme.is_alpha, lexeme.is_digit, lexeme.is_title, lexeme.lang_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
